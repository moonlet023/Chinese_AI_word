import os
import argparse
import cv2
import json
import time
import numpy as np
from pathlib import Path
from datetime import datetime

# TensorFlow å°å…¥
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    print("âœ… TensorFlow æˆåŠŸè¼‰å…¥")
except ImportError as e:
    print(f"âŒ TensorFlow è¼‰å…¥å¤±æ•—: {e}")
    exit(1)

# scikit-learn å°å…¥
try:
    from sklearn.model_selection import train_test_split, StratifiedKFold
    from sklearn.metrics import classification_report
    print("âœ… scikit-learn æˆåŠŸè¼‰å…¥")
except ImportError:
    print("âš ï¸ scikit-learn æœªå®‰è£")

class OptimizedTrainer:
    """å„ªåŒ–çš„è¨“ç·´å™¨ï¼Œå°ˆç‚ºé«˜æº–ç¢ºç‡è¨­è¨ˆ"""
    
    def __init__(self, img_size=256, num_classes=20, batch_size=16, learning_rate=1e-3, class_filter=None):
        self.img_size = img_size
        self.num_classes = num_classes
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        # è‹¥æä¾› class_filterï¼ˆæŒ‡å®šè¦ç”¨å“ªäº›é¡åˆ¥åï¼‰ï¼Œå°‡åœ¨è¼‰å…¥æ™‚å¥—ç”¨
        self.class_filter = set(class_filter) if class_filter else None
        
    def load_optimized_data(self, sample_dir, test_dir, samples_per_class=300):
        """è¼‰å…¥å„ªåŒ–çš„è³‡æ–™é›†"""
        print("ğŸ“‚ è¼‰å…¥å„ªåŒ–è³‡æ–™é›†...")
        
        # è¼‰å…¥è¨“ç·´è³‡æ–™
        X_train, y_train, class_names = self._load_from_dir(
            sample_dir, self.num_classes, samples_per_class
        )
        
        # è¼‰å…¥æ¸¬è©¦è³‡æ–™
        X_test, y_test, _ = self._load_from_dir(
            test_dir, self.num_classes, samples_per_class//2
        )
        
        if X_train is None or X_test is None:
            return None, None, None, None, None
            
        print(f"ğŸ“Š è³‡æ–™çµ±è¨ˆ:")
        print(f"   è¨“ç·´é›†: {len(X_train):,} å¼µåœ–ç‰‡")
        print(f"   æ¸¬è©¦é›†: {len(X_test):,} å¼µåœ–ç‰‡")
        print(f"   é¡åˆ¥æ•¸: {len(class_names)}")
        print(f"   åœ–ç‰‡å°ºå¯¸: {self.img_size}x{self.img_size}")
        
        return X_train, y_train, X_test, y_test, class_names
    
    def _load_from_dir(self, data_dir, max_classes, samples_per_class):
        """å¾ç›®éŒ„è¼‰å…¥è³‡æ–™"""
        data_path = Path(data_dir)
        if not data_path.exists():
            print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
            return None, None, None
        
        character_dirs = sorted([d for d in data_path.iterdir() if d.is_dir()])
        # è‹¥æŒ‡å®š class_filterï¼Œåƒ…ä¿ç•™æŒ‡å®šåç¨±çš„è³‡æ–™å¤¾
        if self.class_filter:
            character_dirs = [d for d in character_dirs if d.name in self.class_filter]
        # å†æˆªæ–·åˆ° max_classes
        character_dirs = character_dirs[:max_classes]
        
        images = []
        labels = []
        class_names = []
        
        for class_idx, char_dir in enumerate(character_dirs):
            class_names.append(char_dir.name)
            image_files = list(char_dir.glob("*.jpg")) + list(char_dir.glob("*.png"))
            
            # éš¨æ©Ÿé¸å–æ¨£æœ¬ä»¥ç¢ºä¿å¤šæ¨£æ€§
            if len(image_files) > samples_per_class:
                np.random.shuffle(image_files)
                image_files = image_files[:samples_per_class]
            
            for img_file in image_files:
                try:
                    img = cv2.imread(str(img_file), cv2.IMREAD_GRAYSCALE)
                    if img is None:
                        continue
                    
                    # é«˜è³ªé‡åœ–ç‰‡é è™•ç†
                    img = self._preprocess_image(img)
                    images.append(img)
                    labels.append(class_idx)
                    
                except Exception as e:
                    continue
        
        if not images:
            return None, None, None
        
        X = np.array(images)
        y = np.array(labels)
        X = np.expand_dims(X, axis=-1)
        
        return X, y, class_names
    
    def _preprocess_image(self, img):
        """é«˜è³ªé‡åœ–ç‰‡é è™•ç†"""
        # 1. å»å™ª
        img = cv2.medianBlur(img, 3)
        
        # 2. å°æ¯”åº¦å¢å¼·
        img = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(img)
        
        # 3. é«˜è³ªé‡ç¸®æ”¾
        img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_CUBIC)
        
        # 4. æ­£è¦åŒ–
        img = img.astype(np.float32) / 255.0
        
        return img
    
    def create_ultra_model(self):
        """å‰µå»ºè¶…é«˜æ€§èƒ½æ¨¡å‹"""
        inputs = keras.Input(shape=(self.img_size, self.img_size, 1))
        
        # åˆå§‹ç‰¹å¾µæå–
        x = layers.Conv2D(32, 7, strides=2, padding='same', activation='relu')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)
        
        # Block 1
        x = self._conv_block(x, 64, 3)
        x = self._conv_block(x, 64, 3)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(0.25)(x)
        
        # Block 2
        x = self._conv_block(x, 128, 3)
        x = self._conv_block(x, 128, 3)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(0.25)(x)
        
        # Block 3
        x = self._conv_block(x, 256, 3)
        x = self._conv_block(x, 256, 3)
        x = self._conv_block(x, 256, 3)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(0.25)(x)
        
        # Block 4
        x = self._conv_block(x, 512, 3)
        x = self._conv_block(x, 512, 3)
        x = self._conv_block(x, 512, 3)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(0.25)(x)
        
        # Block 5
        x = self._conv_block(x, 1024, 3)
        x = self._conv_block(x, 1024, 3)
        
        # å…¨åŸŸå¹³å‡æ± åŒ–
        x = layers.GlobalAveragePooling2D()(x)
        
        # åˆ†é¡å™¨
        x = layers.Dense(2048, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.5)(x)
        
        x = layers.Dense(1024, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.5)(x)
        
        x = layers.Dense(512, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name='ultra_cnn')
        return model
    
    def _conv_block(self, x, filters, kernel_size):
        """å·ç©å¡Š"""
        x = layers.Conv2D(filters, kernel_size, padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        return x
    
    def create_advanced_augmentation(self):
        """å‰µå»ºé«˜ç´šæ•¸æ“šå¢å¼·"""
        return ImageDataGenerator(
            rotation_range=20,
            width_shift_range=0.15,
            height_shift_range=0.15,
            shear_range=0.15,
            zoom_range=0.15,
            brightness_range=[0.8, 1.2],
            channel_shift_range=0.1,
            horizontal_flip=False,
            vertical_flip=False,
            fill_mode='nearest'
        )
    
    def train_ultra_model(self, X_train, y_train, X_test, y_test, epochs=100):
        """è¨“ç·´è¶…é«˜æ€§èƒ½æ¨¡å‹"""
        print("\nğŸš€ é–‹å§‹è¨“ç·´è¶…é«˜æ€§èƒ½æ¨¡å‹")
        print(f"ğŸ¯ ç›®æ¨™æº–ç¢ºç‡: 80%+")
        
        # å‰µå»ºæ¨¡å‹
        model = self.create_ultra_model()
        
        # ç·¨è­¯æ¨¡å‹
        model.compile(
            optimizer=Adam(learning_rate=self.learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"ğŸ“Š æ¨¡å‹åƒæ•¸æ•¸é‡: {model.count_params():,}")
        
        # åˆ†å‰²é©—è­‰é›†
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        # è¨­ç½®å›èª¿
        callbacks = [
            ReduceLROnPlateau(
                monitor='val_accuracy',
                factor=0.3,
                patience=8,
                min_lr=1e-7,
                verbose=1
            ),
            EarlyStopping(
                monitor='val_accuracy',
                patience=20,
                restore_best_weights=True,
                verbose=1
            ),
            # ä½¿ç”¨ HDF5 æ ¼å¼é¿å… native Keras æ ¼å¼èˆ‡å¾Œç«¯ options è¡çªéŒ¯èª¤
            ModelCheckpoint(
                'ultra_best_model.h5',
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=False,  # ä»ä¿å­˜å®Œæ•´æ¨¡å‹
                verbose=1
            )
        ]
        
        # å‰µå»ºæ•¸æ“šå¢å¼·
        datagen = self.create_advanced_augmentation()
        datagen.fit(X_train_split)
        
        print("ğŸ”„ é–‹å§‹è¨“ç·´ï¼ˆä½¿ç”¨é«˜ç´šæ•¸æ“šå¢å¼·ï¼‰...")
        start_time = time.time()
        
        # è¨“ç·´æ¨¡å‹
        history = model.fit(
            datagen.flow(X_train_split, y_train_split, batch_size=self.batch_size),
            steps_per_epoch=max(1, len(X_train_split) // self.batch_size),
            epochs=epochs,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = time.time() - start_time
        
        # è©•ä¼°æ¨¡å‹
        print("\nğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½...")
        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹ï¼ˆæ”¹ç”¨ HDF5 æ ¼å¼ä»¥ç¹é native Keras format çš„ options è¡çªï¼‰
        model.save('ultra_final_model.h5')
        
        print(f"\nâœ… è¨“ç·´å®Œæˆ!")
        print(f"ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")
        print(f"â±ï¸ è¨“ç·´æ™‚é–“: {training_time/60:.1f} åˆ†é˜")
        
        # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
        if test_accuracy >= 0.8:
            print("ğŸ‰ æ­å–œï¼æ¨¡å‹é”åˆ°äº† 80% æº–ç¢ºç‡ç›®æ¨™ï¼")
            print("ğŸ† æ‚¨çš„æ¨¡å‹å·²ç¶“å¯ä»¥æŠ•å…¥ä½¿ç”¨äº†ï¼")
        else:
            print("âš ï¸ å°šæœªé”åˆ° 80% ç›®æ¨™æº–ç¢ºç‡")
            print(f"ğŸ“ˆ é‚„éœ€è¦æå‡ {(0.8 - test_accuracy)*100:.1f} å€‹ç™¾åˆ†é»")
        
        return {
            'test_accuracy': test_accuracy,
            'test_loss': test_loss,
            'training_time': training_time,
            'epochs_trained': len(history.history['accuracy']),
            'target_achieved': test_accuracy >= 0.8,
            'model_file': 'ultra_final_model.h5'
        }

def main():
    """ä¸»å‡½æ•¸ - åŸ·è¡Œé«˜æº–ç¢ºç‡è¨“ç·´"""
    parser = argparse.ArgumentParser(description='è¶…é«˜æ€§èƒ½æ‰‹å¯«æ¨¡å‹è¨“ç·´å™¨')
    parser.add_argument('--img-size', type=int, default=256, help='è¼¸å…¥å½±åƒå°ºå¯¸ (é è¨­ 256)')
    parser.add_argument('--num-classes', type=int, default=20, help='è¨“ç·´çš„é¡åˆ¥æ•¸é‡ä¸Šé™')
    parser.add_argument('--samples-per-class', type=int, default=500, help='æ¯é¡è¨“ç·´æ¨£æœ¬ä¸Šé™')
    parser.add_argument('--epochs', type=int, default=100, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, default=16, help='batch sizeï¼ˆé è¨­ 16ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-3, help='å­¸ç¿’ç‡ï¼ˆé è¨­ 1e-3ï¼‰')
    parser.add_argument('--resume', action='store_true', help='è‹¥å­˜åœ¨ ultra_best_model.h5ï¼Œå¾è©²æª”æ¡ˆç¹¼çºŒè¨“ç·´')
    parser.add_argument('--classes', type=str, default='', help='æŒ‡å®šè¦è¨“ç·´çš„é¡åˆ¥ï¼ˆä»¥é€—è™Ÿåˆ†éš”çš„è³‡æ–™å¤¾åï¼‰')
    parser.add_argument('--train-dir', type=str, default='./data/sample', help='è¨“ç·´è³‡æ–™å¤¾')
    parser.add_argument('--test-dir', type=str, default='./data/test', help='æ¸¬è©¦è³‡æ–™å¤¾')
    args = parser.parse_args()

    print("ğŸš€ é«˜æº–ç¢ºç‡ç¹é«”ä¸­æ–‡æ‰‹å¯«è­˜åˆ¥è¨“ç·´å™¨")
    print("=" * 50)
    print("ğŸ¯ å°ˆé–€è¨­è¨ˆä¾†é”åˆ° 80% ä»¥ä¸Šæº–ç¢ºç‡")
    print(f"ğŸ“ ä½¿ç”¨ {args.img_size}x{args.img_size} é«˜è§£æåº¦åœ–ç‰‡")
    if args.classes:
        cls_list = [c.strip() for c in args.classes.split(',') if c.strip()]
        print(f"ğŸ”¢ åªè¨“ç·´æŒ‡å®šé¡åˆ¥: {cls_list}")
    else:
        print(f"ğŸ”¢ å°ˆæ³¨æ–¼ {args.num_classes} å€‹å­—ç¬¦ï¼ˆä¾è³‡æ–™å¤¾é †åºæˆªå–ï¼‰")
    print("=" * 50)
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    class_filter = [c.strip() for c in args.classes.split(',')] if args.classes else None
    trainer = OptimizedTrainer(
        img_size=args.img_size,
        num_classes=args.num_classes,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        class_filter=class_filter
    )
    
    # è¼‰å…¥è³‡æ–™
    X_train, y_train, X_test, y_test, class_names = trainer.load_optimized_data(
        args.train_dir, args.test_dir, samples_per_class=args.samples_per_class
    )
    
    if X_train is None:
        print("âŒ è³‡æ–™è¼‰å…¥å¤±æ•—")
        return
    
    # è¨“ç·´æ¨¡å‹
    # å¯é¸ï¼šå¾æœ€ä½³æª¢æŸ¥é»æ¢å¾©ï¼ˆè‹¥æ¶æ§‹ä¸€è‡´ï¼‰
    if args.resume and os.path.exists('ultra_best_model.h5'):
        try:
            print('ğŸ” å˜—è©¦å¾ ultra_best_model.h5 ç¹¼çºŒè¨“ç·´...')
            # è¼‰å…¥æ•´å€‹æ¨¡å‹ï¼ˆéœ€æ¶æ§‹ä¸€è‡´ï¼‰
            prev_model = tf.keras.models.load_model('ultra_best_model.h5')
            # ç›´æ¥ç”¨å…ˆå‰æ¨¡å‹è©•ä¼°ä¸¦å¦è¡Œè¨“ç·´ï¼ˆæ­¤ç¤ºä¾‹ä¿æŒç°¡å–®ï¼Œä»æ²¿ç”¨ trainer è¨­å®šï¼‰
        except Exception as e:
            print('âš ï¸ æ¢å¾©è¨“ç·´å¤±æ•—ï¼Œå°‡å¾é ­é–‹å§‹ï¼š', e)
    result = trainer.train_ultra_model(X_train, y_train, X_test, y_test, epochs=args.epochs)
    
    # ä¿å­˜çµæœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"ultra_training_results_{timestamp}.json"
    
    # å°‡é¡åˆ¥åç¨±ä¹Ÿå¯«å…¥çµæœï¼Œæ–¹ä¾¿ Web èˆ‡è©•ä¼°å°æ‡‰ indexâ†’label
    if class_names:
        result_with_classes = dict(result)
        result_with_classes['class_names'] = class_names
    else:
        result_with_classes = result

    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(result_with_classes, f, ensure_ascii=False, indent=2)

    # å¦å­˜ä¸€ä»½ç°¡å–®çš„é¡åˆ¥æ¸…å–®æª”ï¼Œåˆ©æ–¼ç¨ç«‹æŸ¥é–±/è¼‰å…¥
    try:
        classes_sidecar = {
            'model_file': result.get('model_file', 'ultra_final_model.h5'),
            'class_names': class_names if class_names else [],
            'generated_at': timestamp
        }
        with open('ultra_classes.json', 'w', encoding='utf-8') as f:
            json.dump(classes_sidecar, f, ensure_ascii=False, indent=2)
        print('ğŸ“„ å·²è¼¸å‡ºé¡åˆ¥æ¸…å–® -> ultra_classes.json')
    except Exception as e:
        print('âš ï¸ è¼¸å‡º ultra_classes.json å¤±æ•—:', e)
    
    print(f"\nğŸ’¾ çµæœå·²ä¿å­˜: {results_file}")
    
    if result['target_achieved']:
        print("\nğŸ‰ æˆåŠŸï¼æ‚¨çš„æ¨¡å‹å·²é”åˆ° 80% æº–ç¢ºç‡ç›®æ¨™ï¼")
        print("ğŸ’¡ ç¾åœ¨å¯ä»¥æ›´æ–° Web æ‡‰ç”¨ç¨‹åºä½¿ç”¨æ–°æ¨¡å‹äº†")
        print(f"ğŸ“ æ–°æ¨¡å‹æª”æ¡ˆ: {result['model_file']}")
    else:
        print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
        print("1. ğŸ”„ ç¹¼çºŒè¨“ç·´æ›´å¤š epochs")
        print("2. ğŸ“Š å¢åŠ æ¯å€‹é¡åˆ¥çš„è¨“ç·´æ¨£æœ¬")
        print("3. ğŸ¯ é€²ä¸€æ­¥æ¸›å°‘é¡åˆ¥æ•¸é‡")
        print("4. ğŸ”§ èª¿æ•´æ¨¡å‹æ¶æ§‹æˆ–è¶…åƒæ•¸")

if __name__ == "__main__":
    main()